NB : i mean an indice
Extract 15 equidistant frames
Apply FastMobileEnhancer (CLAHE) to those frames

If frame i (0–14) lacks hand detection, linear interpolate using nearest valid frames within ±3 indices (max(0, i-3) to min(14, i+3)). No distant smoothing across sign boundaries—preserves trajectories. Long gaps (>6 frames) → zero pad + mask=0.

Data format [15,134]
[(left/right_hand_bone_vectors from 3d hand model), (shoulder-midpoint_to_left/right_pose_wrist from pose models),
(left/right_wrist_velocity_pose_model),(left_hand_present, right_hand_present are masks for 3d hand model)] 

Bone vectors :
Left hand processing → use LEFT wrist/scale exclusively
Right hand processing → use RIGHT wrist/scale exclusively
Wrist‑centric translation
Scale normalization (wrist to middle fingertip)
Build 20 bone vectors (Use MediaPipe's fixed hand skeleton graph (parent→child connections))

Vectors coming from midpoint-shoulder :
Compute raw vectors
Normalize by shoulder width

left/right_wrist_velocity_pose_model :

Since your position vectors (shoulder_mid → wrist) 
are already torso‑scale normalized (/ shoulder_width), 
Compute Δ on torso‑normalized positions (norm_pos[t] - norm_pos[t-1]). Frame 0 = zeros. Frame‑rate + signer‑speed invariant. Interpolate wrist positions first if missing.

Hand presence masks (hands only):
[left_present, right_present] = 1.0 if multi_hand_world_landmarks detected, else 0.0 (even post‑interp).

---

import cv2
import numpy as np

class FastMobileEnhancer:
    def __init__(self):
        # 1. Initialize CLAHE once. 
        # Clip limit 1.5 is the safe zone.
        # Tile grid 4x4 (instead of 8x8) is much faster on mobile CPUs 
        # while still providing excellent local contrast for hand tracking.
        self.clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(4, 4))

    def apply(self, frame: np.ndarray) -> np.ndarray:
        """
        Memory & CPU optimized CLAHE for real-time mobile inference.
        Returns an RGB image ready directly for MediaPipe.
        """
        # OPTIMIZATION 1: Use YUV instead of LAB. 
        # Y is the luminance (brightness) channel. 
        # YUV conversion is mathematically lighter on ARM CPUs than LAB.
        yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)

        # OPTIMIZATION 2: In-place memory operation.
        # Instead of splitting all 3 channels into new memory arrays (cv2.split),
        # we extract only the Y channel via slicing. This saves RAM allocations.
        y_channel = yuv_frame[:, :, 0]

        # Apply CLAHE directly to the Y channel, overwriting it in-place
        yuv_frame[:, :, 0] = self.clahe.apply(y_channel)

        # OPTIMIZATION 3: Convert directly to RGB for MediaPipe.
        # Standard pipeline is BGR -> YUV -> BGR -> RGB (2 conversions).
        # We skip the BGR middleman and go YUV -> RGB directly.
        rgb_frame = cv2.cvtColor(yuv_frame, cv2.COLOR_YUV2RGB)
        
        return rgb_frame

# ==========================================
# HOW TO USE IN YOUR SCRIPT
# ==========================================
# 1. Initialize outside the loop
# enhancer = FastMobileEnhancer()
#
# 2. Inside your camera loop:
# ret, frame = cap.read()
# 
# # Apply the fast enhancement (returns RGB)
# rgb_frame = enhancer.apply(frame)
# 
# # Send directly to MediaPipe (MediaPipe requires RGB)
# results = mediapipe_hands.process(rgb_frame)
